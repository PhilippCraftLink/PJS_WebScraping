import subprocess
import os
import json
import time
from pymongo import ASCENDING, UpdateOne

"""
This module contains functions to run Scrapy spiders for scraping job listings from Stepstone and to save the scraped data to a MongoDB database.
It handles waiting for spider output files, saving data to MongoDB, and managing file paths for the scraped data.
"""

def wait_for_file(json_file, timeout=30):
    """
    Wait for a specified file to be created within a given timeout period.

    This function is used to wait for the output file generated by a Scrapy spider. It checks periodically if the file exists
    and returns once the file is found or the timeout is reached.

    :param json_file: The path to the file to wait for.
    :param timeout: The maximum time to wait in seconds (default is 30 seconds).
    :return: True if the file is found within the timeout, False otherwise.
    """
    print(f"⏳ Waiting for file: {json_file}...")
    start_time = time.time()
    while not os.path.exists(json_file):
        if time.time() - start_time > timeout:
            print(f"❌ Timeout: {json_file} not found!")
            return False
        time.sleep(1)
    return True

def save_to_mongo(json_file, job_title, db):
    """
    Save the scraped job data from a JSON file to a MongoDB collection.

    This function reads the JSON file produced by the Scrapy spider, processes the data, and inserts or updates the records
    in the specified MongoDB collection. It handles both single job entries and lists of jobs.

    :param json_file: The path to the JSON file containing the scraped job data.
    :param job_title: The job title used to determine the MongoDB collection name.
    :param db: A MongoDB database instance to store the data.
    """
    if not wait_for_file(json_file):
        return

    try:
        collection_name = job_title.replace(" ", "_").lower()
        collection = db["stepstone_" + collection_name]

        if "jobId_1" not in collection.index_information():
            collection.create_index([("jobId", ASCENDING)], unique=True)

        with open(json_file, "r", encoding="utf-8") as file:
            data = json.load(file)

            if isinstance(data, list):
                bulk_ops = [
                    UpdateOne({"jobId": item["jobId"]}, {"$set": item}, upsert=True)
                    for item in data
                ]
                if bulk_ops:
                    collection.bulk_write(bulk_ops, ordered=False)
            else:
                collection.update_one(
                    {"jobId": data["jobId"]},
                    {"$set": data},
                    upsert=True
                )

        print(f"✅ Data saved/updated in '{collection_name}'")

    except Exception as e:
        print(f"❌ Critical error: {e}")

def get_latest_output_file(directory, job_title):
    """
    Retrieve the most recent JSON output file for a given job title in the specified directory.

    This function lists all JSON files in the directory that start with the job title and selects the most recently created one.

    :param directory: The directory to search for JSON files.
    :param job_title: The job title used to filter the files.
    :return: The path to the most recent JSON file, or None if no files are found.
    """
    files = [f for f in os.listdir(directory) if f.startswith(job_title) and f.endswith(".json")]
    if not files:
        return None
    files.sort(reverse=True)
    return os.path.join(directory, files[0])

def run_spiders(job_title, db):
    """
    Run Scrapy spiders to scrape job listings from Stepstone and save the data to MongoDB.

    This function sets up and runs two Scrapy spiders: one to collect job links and another to scrape detailed job information.
    It manages the output files and ensures the scraped data is saved to the appropriate MongoDB collection.

    :param job_title: The job title to search for on Stepstone.
    :param db: A MongoDB database instance to store the scraped data.

    the project_path is defined as follows for the Docker configuration: ‘/app/stepstonesearch’,
    in the case of local execution this must be adapted accordingly (localpath/stepstonesearch)
    """
    project_path = "/app/stepstonesearch"
    links_output_file = f"{project_path}/links_output.json"

    if os.path.exists(links_output_file):
        os.remove(links_output_file)

    subprocess.run(
        ["scrapy", "crawl", "Links", "-o", links_output_file, "-a", f"job_title={job_title}"],
        cwd=project_path
    )

    subprocess.run(
        ["scrapy", "crawl", "sitespider", "-a", f"input_file={links_output_file}", "-a", f"job_title={job_title}"],
        cwd=project_path
    )

    time.sleep(2)
    job_details_file = get_latest_output_file(project_path, job_title)
    if job_details_file:
        save_to_mongo(job_details_file, job_title, db)